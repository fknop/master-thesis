% !TEX root = ../../thesis.tex

\documentclass[../../thesis.tex]{subfiles}
 
\begin{document}

A \emph{Constraint Satisfaction Problem} (CSP) consists of a set of $n$ variable, 
$\{x_1, \dots, x_n \}$; a domain $D(x_i)$ of possible values for each variable $x_i$, 
$1 \leq i \leq n$; and a collection of $m$ constraints $\{ C_1, \dots, C_m \}$. 
Each constraint $C_j$, $1 \leq j \leq m$, is a constraint over some set of variables called the scheme 
of the constraint. The size of this set is known as the arity of the constraint. 
A solution to a CSP is an assignment of values $a_i \in D_i$ to $x_i$, that satisfies all of the constraints. \cite{cp-definition}

\subsection{Global Constraints}

As described in more depth in \cite{Hentenryck:2003}:

\begin{quotation}
  [\dots] a constraint $C$ is often called “global” when “processing” $C$ as a whole gives better results than “processing” any conjunction
  of constraints that is “semantically equivalent” to $C$.
\end{quotation}

The author also define three types of constraint globality, we are mostly interested in what he refers to \emph{operatinal globality}. 
Those constraints can be decomposed into multiple simpler constraints but the filtering quality of the decomposition
is often worse than its global counterpart. 

There also exists soft variants \cite{Regin:2000} of global constraints where a constraint is associated with 
a number of violations. This is particularely useful for over-constrained problems which cannot be solved by a CSP.
Instead the CSP is transformed into a \emph{Constraint Optimization Problem} (COP) where we minimize the number of violations.
\subsubsection{AllDifferent Cconstraint}
\label{sota:alldifferent}

The \texttt{allddifferent} constraint \cite{Rgin1994AFA} is one of of the most famous global constraint used in Constraint 
Programming.
This constraint is defined over a subset of variables for which values must be different. More formally:

\begin{equation*}
  \texttt{alldifferent}(x_1, \dots, x_n) = \{ (d_1, \dots, d_n) \mid d_i \in D(x_i), d_i \neq d_j \forall i \neq j \}
\end{equation*}

This constraint can be decomposed into multiple binary inequalities. It makes \texttt{alldifferent} an operational global constraint.
It can be proven that the filtering of the global constraint cannot be achieved with a decomposition. As 
an example, let's define three variables $x_1$, $x_2$ and $x_3$ respectively taking domains $\{1,2\}$, $\{1,2\}$, $\{1,2,3,4\}$. 
The global constraint would be able to successfuly filter $1$ and $2$ from the domain of 
$x_3$ because the values are always taken by $x_1$ and $x_2$. However, the decomposition is not able to filter those values.


\subsubsection{Global Cardinality Constraint}
\label{sota:gcc}

The global cardility constraint (\texttt{gcc}) \cite{Regin:1996} is a generalization of the 
\texttt{alldifferent} constraint. It does not enforces (although it can) the uniqueness of values of its variables
but instead enforces that the cardinality of each value $d_i$ for all its variables in its scope lies
between a lowerbound and a upperbound, respectively $l_i$ and $u_i$. 

\begin{equation*}
  \texttt{gcc}(X, l, u) = \{ (d_1, \dots, d_n) \mid d_i \in D(x_i), l_d \leq |\{ d_i \mid d_i = d \}| \leq u_d, \forall d \in D(X) \}
\end{equation*}

As stated above, we can express the \texttt{alldifferent} constraint with this definition:

\begin{equation*}
\texttt{gcc}(\{ x_1, \dots, x_n \}, [1, \dots, 1], [1, \dots, 1]) = \texttt{alldifferent}(x_1, \dots, x_n)
\end{equation*}


We are also interested in a soft variant of \texttt{gcc} called \texttt{softgcc} \cite{VanHoeve2006}. 
The violation associated with this constraint is the sum of excess or shortage \cite{schaus:softgcc} for each value.

\begin{align*}
  \texttt{softgcc}(X, l, u, Z) &= \{ (d_1, \dots, d_n) \mid d_i \in D(x_i), d_z \in D(Z), viol(d_1, \dots, d_n) \leq d_z \} \\
  \text{with} \quad viol(d_1, \dots, d_n) &= \sum_{d \in D(X)} \text{max}(0, |\{ d_i \mid d_i = d \}| - u_d, l_d - |\{ d_i \mid d_i = d \}|)
\end{align*}


\subsection{Large Neighorhood Search}

A Constraint Programming solver can often get stuck in a search tree that do not lead to good solutions.
We want instead to explore as much of the search space as possible. 

Large Neighborhood Search (LNS) is a technique that makes use of the principles of \emph{Local Search}.
LNS uses Constraint Programming as a tool to find solutions and local search to expand the exploration of the search space. 
The LNS framework often goes as follow:

\begin{enumerate}
  \item Use Constraint Programming to find a solution 
  \item Relax last best solution: we fix some variables to the last value in the best solutions. This is the 
        step that can change the most for different types of problems. Most of the time, a simple random relaxation is used (i.e. fix a percentage of variable).
  \item Restart
\end{enumerate}

The entire search might be limited with a time limit, number of solutions or number of restarts. Each independant search 
is often limited with a number of backtracks or a time limit.

\subsection{Variable Objective Search}

A multi-objective problem is often modeled by having a weighted sum of sub-objectives to form a 
single objective.

\begin{align*}
  \text{min} \quad & obj = \sum_{i} w_i o_i \\
  \text{s.t.} \quad & constraints
\end{align*}

Variable Objective Large Neighborhood Search (VO-LNS) \cite{Schaus:VOLNS} is an extension of LNS for multi-objective 
problems which offers
\begin{enumerate*}[label=(\roman*)]
  \item Prioritization of sub-objectives;
  \item Better pruning.
\end{enumerate*}
VO-LNS consists of three types of filtering for each objective
\begin{enumerate}
  \item \emph{No-Filtering}: The objective has no impact.
  \item \emph{Weak-Filtering}: When a solution is found, it has to be better or equal to the bound of the objective. 
  \item \emph{Strong-Filtering}: When a solution is found, it has to be strictly improving the bound of the objective.
\end{enumerate}

The VO-LNS formulation is expressed as follows:

\begin{align*}
  \text{min} \quad & obj = (obj_1, \dots, obj_n, obj_{n+1}) \\
  \text{s.t.} \quad & constraints 
\end{align*}

$obj_1, \dots, obj_n$ are the sub-objectives while $obj_{n+1}$ is the sum of all sub-objectives. We keep $obj_{n+1}$ in 
\emph{Strong-Filtering} during the search such that the formulation is at least as strong as a sum of sub-objectives.
We can change the filtering dynamically during the search before each restart depending on the problem and prioritization of sub-objectives.

\end{document}

