% !TEX root = ../../thesis.tex

\documentclass[../../thesis.tex]{subfiles}
 
\begin{document}

Constraint Programming is a technique used for solving hard combinatorial problems. It is a programming paradigm 
where relations between variables are stated as constraints to create a Constraint Satisfaction Problem.

\paragraph{}

A \emph{Constraint Satisfaction Problem} (CSP) consists of a set of $n$ variable, 
$\{x_1, \dots, x_n \}$; a domain $D(x_i)$ of possible values for each variable $x_i$, 
$1 \leq i \leq n$; and a collection of $m$ constraints $\{ C_1, \dots, C_m \}$. 
Each constraint $C_j$, $1 \leq j \leq m$, is a constraint over some set of variables called the scheme 
of the constraint. The size of this set is known as the arity of the constraint. 
A solution to a CSP is an assignment of values $a_i \in D_i$ to $x_i$, that satisfies all the constraints. \cite{cp-definition}

Problems are sometimes over-constrained and hard to solve with a CSP. In those cases, we can transform our 
CSP to a Constraint Optimization Problem (COP) where we try to optimize one or multiple objectives coming 
from the transformation of hard constraints into soft constraints.

\subsection{Global constraints}


We now describe the principle of global constraints and how they are useful for our resource allocation problem 
described in \autoref{chapter:problem}.

As described in more depth in \cite{Hentenryck:2003}: 

\begin{quotation}
  [\dots] a constraint $C$ is often called “global” when “processing” $C$ as a whole gives better results than “processing” any conjunction
  of constraints that is “semantically equivalent” to $C$.
\end{quotation}

The author also defines three types of constraint globality, we are mostly interested in what he refers to \emph{operational globality}. 
Those constraints can be decomposed into multiple simpler constraints but the filtering quality of the decomposition
is often worse than its global counterpart. 

There also exists soft variants \cite{Regin:2000} of global constraints where a constraint is associated with 
a number of violations. This is particularly useful for over-constrained problems which cannot be solved by a CSP.
Instead, the CSP is transformed into a \emph{Constraint Optimization Problem} (COP) where the number of violations is minimized.
\subsubsection{Alldifferent constraint}
\label{sota:alldifferent}

In our resource allocation problem (\autoref{chapter:problem}), we need to assign different workers to demands during the same time period. 
This is usually done in Constraint Programming by using the \texttt{alldifferent} constraint.

\paragraph{}

The \texttt{alldifferent} constraint \cite{Rgin1994AFA} is one of the most famous global constraint used in Constraint 
Programming.
This constraint is defined over a subset of variables for which values must be different. More formally:

\begin{equation*}
  \texttt{alldifferent}(x_1, \dots, x_n) = \{ (d_1, \dots, d_n) \mid d_i \in D(x_i), d_i \neq d_j \forall i \neq j \}
\end{equation*}

This constraint can be decomposed into multiple binary inequalities. This makes \texttt{alldifferent} an operational global constraint.
It can be proven that the filtering of the global constraint cannot be achieved with a decomposition. As 
an example, let us define three variables $x_1$, $x_2$ and $x_3$ respectively taking domains $\{1,2\}$, $\{1,2\}$, $\{1,2,3,4\}$. 
The global constraint would be able to successfully filter $1$ and $2$ from the domain of 
$x_3$ because the values are always taken by $x_1$ and $x_2$. However, the decomposition is not able to filter those values.


\subsubsection{Global cardinality constraint}
\label{sota:gcc}

As described in \autoref{chapter:problem}, our resource allocation problem needs to take into account working requirements, i.e. a minimum 
and a maximum number of times that a worker can work. For this, we need to count the occurrences of a worker being assigned to a position and 
limit those occurrences to a minimum and maximum. 

\paragraph{}


The global cardinality constraint (\texttt{gcc}) \cite{Regin:1996} is a generalization of the 
\texttt{alldifferent} constraint. It does not enforce (although it can) the uniqueness of values of its variables
but enforces that the cardinality of each value $d_i$ for all its variables in its scope lies
between a lower bound and an upper bound, respectively $l_i$ and $u_i$. 

\begin{equation*}
  \texttt{gcc}(X, l, u) = \{ (d_1, \dots, d_n) \mid d_i \in D(x_i), l_d \leq |\{ d_i \mid d_i = d \}| \leq u_d, \forall d \in D(X) \}
\end{equation*}

As stated above, we can express the \texttt{alldifferent} constraint with this definition:

\begin{equation*}
\texttt{gcc}(\{ x_1, \dots, x_n \}, [1, \dots, 1], [1, \dots, 1]) = \texttt{alldifferent}(x_1, \dots, x_n)
\end{equation*}


We are also interested in a soft variant of \texttt{gcc} called \texttt{softgcc} \cite{VanHoeve2006}. 
The violation associated with this constraint is the sum of excess or shortage \cite{schaus:softgcc} for each value.

\begin{align*}
  \texttt{softgcc}(X, l, u, Z) &= \{ (d_1, \dots, d_n) \mid d_i \in D(x_i), d_z \in D(Z), viol(d_1, \dots, d_n) \leq d_z \} \\
  \text{with} \quad viol(d_1, \dots, d_n) &= \sum_{d \in D(X)} \text{max}(0, |\{ d_i \mid d_i = d \}| - u_d, l_d - |\{ d_i \mid d_i = d \}|)
\end{align*}


\subsection{Large Neighborhood Search}

A Constraint Programming solver can often get stuck in a search tree that does not lead to good solutions.
We want instead to explore as much of the search space as possible. 

Large Neighborhood Search (LNS) is a technique that makes use of the principles of \emph{Local Search}.
LNS uses Constraint Programming as a tool to find solutions and local search to expand the exploration of the search space. 
The LNS framework often goes as follows:

\begin{enumerate}
  \item Use Constraint Programming to find a solution 
  \item Relax last best solution: we fix some variables to the last value in the best solutions. This is the 
        step that can change the most for different types of problems. Most of the time, a simple random relaxation is used (i.e. fix a percentage of variables).
  \item Restart
\end{enumerate}

The entire search might be limited with a time limit, number of solutions or number of restarts. Each independent search 
is often limited with a number of backtracks or a time limit.

\subsection{Variable Objective Search}

A multi-objective problem is often modeled by having a weighted sum of sub-objectives to form a 
single objective. 

\begin{align*}
  \text{min} \quad & obj = \sum_{i} w_i o_i \\
  \text{s.t.} \quad & constraints
\end{align*}

Our resource allocation problem (\autoref{chapter:problem}) uses such an objective. One issue with this objective modeling is how 
to prioritize sub-objectives (e.g. is it more important to minimize contiguous workers or requirements). This is 
usually solved by assigning more weight to more important objectives. However, the pruning of such method alone is inefficient.

\paragraph{}

Variable Objective Large Neighborhood Search (VO-LNS) \cite{Schaus:VOLNS} is an extension of LNS for multi-objective 
problems which offers
\begin{enumerate*}[label=(\roman*)]
  \item prioritization of sub-objectives;
  \item better pruning.
\end{enumerate*}
Each objective in the VO-LNS framework has three types of filtering available:
\begin{enumerate}
  \item \emph{No-Filtering}: The objective has no impact.
  \item \emph{Weak-Filtering}: When a solution is found, it has to be better or equal to the bound of the objective. 
  \item \emph{Strong-Filtering}: When a solution is found, it has to be strictly improving the bound of the objective.
\end{enumerate}

The VO-LNS formulation is expressed as follows:

\begin{align*}
  \text{min} \quad & obj = (obj_1, \dots, obj_n, obj_{n+1}) \\
  \text{s.t.} \quad & constraints 
\end{align*}

$obj_1, \dots, obj_n$ are the sub-objectives while $obj_{n+1}$ is the original weighted sum. We keep $obj_{n+1}$ in 
\emph{Strong-Filtering} during the search such that the formulation is at least as strong as a sum of sub-objectives.
We can change the filtering dynamically during the search before each restart depending on the problem and prioritization of sub-objectives.

\subsection{Heuristics}

The backtracking algorithm uses two heuristics for its search. One heuristic chooses the variable while the other chooses 
the value for the previously selected variable. Good heuristics can drive the search quickly to good solutions. 
We present in \autoref{section:mipmodel} a value heuristic created for the needs of the problem.


One of the biggest principle used for variable ordering is the first-fail principle. 
This principle states that the search should first select the variable that most likely leads to 
an inconsistency. Multiple heuristics follow this principle, the most simple being the smallest domain ordering.




\end{document}

