% !TEX root = ../../thesis.tex

\documentclass[../../thesis.tex]{subfiles}
 
\begin{document}

In this chapter, we experiment our solvers and discuss the results. 

\section{Benchmark process}

Our benchmark API takes an options structure (\autoref{benchmark:options}).
The arrays \texttt{T}, \texttt{D} and \texttt{W} determines the sizes of the instances. The benchmark runner 
will create instances with a combinations of those parameters. For example, with the values in \autoref{benchmark:options},
the benchmark runner will create 6 instances with the sizes: $(5, 30, 100)$, $(5, 50, 100)$, $(5, 30, 200)$, $(5, 50, 200)$, $(5, 30, 300)$ and $(5, 50, 300)$ with the 
three values being $(T, D, W)$.

\begin{lstlisting}[style=scalaStyle,label={benchmark:options},caption={Benchmark options},captionpos=b]
trait BenchmarkOptions {
  val solutionLimit: Int = Int.MaxValue
  val timeLimit: Int = 20
  val repeat: Int = 1
  val dryRun: Int = 1
  val T: Array[Int] = Array(5)
  val D: Array[Int] = Array(30, 50)
  val W: Array[Int] = Array(100, 200, 300)
  val probabilities: Map[String, Double] = Map()
  val seed: Long = -1L
}
\end{lstlisting}

We can specify a solution limit as well as a time limit. We can also repeat our benchmark and takes 
the average of results. We can also have dry runs to warm up the JVM (Java Virtual Machine) and a seed to have reproducible benchmarks. Finally,
we can specify the probabilities for our instances as explained in \autoref{section:instance-gen}.

Our benchmark runner has a function \texttt{run} that takes a name (e.g. solver name, serie name, etc), and a solving function 
which takes a generic model and returns a pair with the time spent in milliseconds and the objective value respectively.

\begin{lstlisting}[style=scalaStyle,label={benchmark:run},caption={Benchmark run function},captionpos=b]
class BenchmarkRunner(val options: BenchmarkOptions) {
  def run (
    name: String, 
    solve: VillageOneModel => (Long, Int)
  ): (BenchmarkSerie, BenchmarkSerie)
}
\end{lstlisting}

This functions returns a pair of benchmark series: one serie for the time values and one serie for the objective values.
The \texttt{BenchmarkSerie} class is simply a class that takes a name and a list of benchmark measurements (i.e. mean, standard deviation, min and max).

This implementation allows us to create a variety of benchmark by simply changing the \texttt{solve} function.

\subsection{Hardware}

All the experiments were performed on a UCLouvain server \cite{jabba} from the INGI department.

\begin{itemize}
  \item Intel(R) Xeon(R) CPU E5-2687W v3 \@ 3.10GHz with 20 cores and 40 threads.
  \item 128 Go RAM
\end{itemize}

\subsection{Benchmark Instances}

For our experiments, we generated 216 instances of different sizes.
\autoref{instances:proportions} shows the size proportions for the 216 instances used in the experiments. $T$ represents the number of time period, $D$ the number of demands and $W$ the number of workers. The generated instances also have varying probabilities as presented in \autoref{instance:options}. These probabilities are referenced in \autoref{instances:probabilities} and follow a uniform distribution where $P_{\mu} = \frac{P_{min} + P_{max}}{2}$.

\begin{table}[H]
  \caption{Proportions of instance sizes for 216 instances}
  \label{instances:proportions}
  \centering
  \begin{tabular}[t]{|c|c|c |c|c|}
    \hline
    \multicolumn{3}{|c|}{Size} & \multicolumn{2}{|c|}{Prop.} \\
    \hline 
    $T$ & $D$ & $W$ & $n$ & $\%$ \\
    \hline 
    \multirow{9}{*}{5} & \multirow{3}{*}{30} & 150 & 8 & 3.7 \\ 
    \cline{3-5}
     &  & 225 & 8 & 3.7 \\ 
     \cline{3-5}
     &  & 300 & 8 & 3.7 \\ 
     \cline{2-5}
     & \multirow{3}{*}{40} & 150 & 8 & 3.7 \\ 
     \cline{3-5}
     &  & 225 & 8 & 3.7 \\ 
     \cline{3-5}
     &  & 300 & 8 & 3.7 \\ 
     \cline{2-5}
     & \multirow{3}{*}{50} & 150 & 8 & 3.7 \\ 
     \cline{3-5}
     &  & 225 & 8 & 3.7 \\ 
     \cline{3-5}
     &  & 300 & 8 & 3.7 \\ 
    \hline
  \end{tabular}
  \hfill
  \begin{tabular}[t]{|c|c|c |c|c|}
    \hline
    \multicolumn{3}{|c|}{Size} & \multicolumn{2}{|c|}{Prop.} \\
    \hline 
    $T$ & $D$ & $W$ & $n$ & $\%$ \\
    \hline 
    \multirow{9}{*}{10} & \multirow{3}{*}{30} & 150 & 8 & 3.7 \\ 
    \cline{3-5}
     &  & 225 & 8 & 3.7 \\ 
     \cline{3-5}
     &  & 300 & 8 & 3.7 \\ 
     \cline{2-5}
     & \multirow{3}{*}{40} & 150 & 8 & 3.7 \\ 
     \cline{3-5}
     &  & 225 & 8 & 3.7 \\ 
     \cline{3-5}
     &  & 300 & 8 & 3.7 \\ 
     \cline{2-5}
     & \multirow{3}{*}{50} & 150 & 8 & 3.7 \\ 
     \cline{3-5}
     &  & 225 & 8 & 3.7 \\ 
     \cline{3-5}
     &  & 300 & 8 & 3.7 \\ 
    \hline
  \end{tabular}
  \hfill
  \begin{tabular}[t]{|c|c|c |c|c|}
    \hline
    \multicolumn{3}{|c|}{Size} & \multicolumn{2}{|c|}{Prop.} \\
    \hline 
    $T$ & $D$ & $W$ & $n$ & $\%$ \\
    \hline 
    \multirow{9}{*}{15} & \multirow{3}{*}{30} & 150 & 8 & 3.7 \\ 
    \cline{3-5}
     &  & 225 & 8 & 3.7 \\ 
     \cline{3-5}
     &  & 300 & 8 & 3.7 \\ 
     \cline{2-5}
     & \multirow{3}{*}{40} & 150 & 8 & 3.7 \\ 
     \cline{3-5}
     &  & 225 & 8 & 3.7 \\ 
     \cline{3-5}
     &  & 300 & 8 & 3.7 \\ 
     \cline{2-5}
     & \multirow{3}{*}{50} & 150 & 8 & 3.7 \\ 
     \cline{3-5}
     &  & 225 & 8 & 3.7 \\ 
     \cline{3-5}
     &  & 300 & 8 & 3.7 \\ 
    \hline
  \end{tabular}
\end{table}


\begin{table}[H]
  \caption{Probability range for generated instances}
  \label{instances:probabilities}
  \centering
  \begin{tabular}[t]{|l r r|}
    \hline 
    $P_{name}$ & $P_{min}$ & $P_{max}$ \\
    \hline
    assignSkill & 0.1 & 0.3 \\
    assignWorkerSkill & 0.1 & 0.3 \\
    assignPeriod & 0.4 & 0.8 \\
    assignLocation & 0.3 & 0.7 \\
    assignMachines & 0.1 & 0.5 \\
    takeMachine & 0.1 & 0.3 \\
    assignWorkingRequirements & 0.1 & 0.3 \\
    assignWWI & 0 & 0.1 \\
    assignWCI & 0 & 0.1 \\
    \hline
  \end{tabular}
\end{table}

\section{Constraint Programming}

\subsection{Comparison between heuristics}

We talked in \autoref{section:cpmodel} about our custom heuristic called Most Available Heuristic.
We now compare the performance of this heuristic with standard heuristic like the Max Value heuristic.
We also compare the variable heuristics used in addition to our aforementioned heuristic.

\subsubsection{Value heuristic}

We compare multiple value heuristics:

\begin{itemize}
  \item The Max Value heuristic. This heuristic simply takes the maximum value in the domain of the variable. We 
  take the maximum instead of the minimum because of the sentinel value being equal to -1 in every domain. 
  \item The Most Available heuristic discussed in \autoref{section:cpmodel}.
  \item The Dynamic Most Available heuristic also discussed in \autoref{section:cpmodel}.
\end{itemize}

We first start by comparing the Max Value and the static Most Available heuristics together.
\autoref{experiments:heuristic1} shows the objective ratio between the implemented custom Most Available heuristic and a standard 
Max Value heuristic after the first solution.
The two heuristics were tested on 72 instances of various sizes from small to big instances.
The performance profile shows a clear gain of about $2$ to $3.4$ for our custom heuristic.

Heuristics like Max Value are ones of the simpler heuristics to implement but often offer bad performance due to 
the lack of knowledge of the problem.


\begin{figure}
  \centering
  \includegraphics[scale=0.55]{experiments/heuristic.png}
  \caption{Most Available and First Fail heuristics [72 instances/first solution].}
  \label{experiments:heuristic1}
\end{figure}

\paragraph{}

We now compare our static and dynamic Most Available heuristics together.
\autoref{experiments:heuristic2} shows the performance profile of the dynamic and static \texttt{mostavailable} heuristics.
We can see that the dynamic version of the heuristic outperforms the static one in every instances.
Even though the dynamic version need to process more during the search, we can see that the time lost 
by this processing is gained back during the search.

\paragraph{}

As our dynamic value heuristic outperforms other tested value heuristics, we will assume for the rest of this chapter 
that the value heuristic for the Constraint Programming solver is the dynamic \texttt{mostavailable}.


\begin{figure}
  \centering
  \includegraphics[scale=0.55]{experiments/static-dynamic-heuristic.png}
  \caption{Most Available: static and dynamic [486 instances/15s].}
  \label{experiments:heuristic2}
\end{figure}


\subsubsection{Variable heuristic}

In addition to our \texttt{mostavailable} heuristic, we use a variation of the \texttt{maxdegree} heuristic. 
This heuristic is a first-fail variable heuristic that selects the most constrained unbound variable. However,
as stated in our model in \autoref{section:cpmodel}, skills are not represented with constraints but instead values 
are removed from the domain at initialization. To express skills as part of the max degree, we simply add the number of 
skills required by a variable to the degree of that variable.


We compare multiple variable heuristics:

\begin{itemize}
  \item Custom Max Degree (MD) heuristic
  \item Min Size (MS) heuristic. The Min Size heuristic simply selects the variable with minimum domain size.
  \item Conflict Ordering Search (COS) heuristic
\end{itemize}


Conflict Ordering Search (COS) \cite{Gay:COS} is a variable ordering heuristic that 
reorders variables based on the number of conflicts that happen during the search.
It is a variant to the Last Conflict heuristic that selects the variable which caused the last conflict first.
COS was shown to be the most performant on scheduling problems. We will now see how it performs in our problem.

\autoref{experiments:heuristics:3} shows a performance profile for the objective after 30s for each solver.
We can see that the three heuristics are very close of each other. However, the Min Size heuristic performs slightly better the Max Degree heuristic with the 
COS heuristic coming last.


\begin{figure}
  \centering
  \includegraphics[scale=0.55]{experiments/cos-md-ms.png}
  \caption{COS, Max Degree and Min Size heuristics [216 instances/30s].}
  \label{experiments:heuristics:3}
\end{figure}

The three heuristics perform well and changing between these heuristics will not show a significant improvement.

TODO: Solution/time

\subsection{Comparison between searches}


As stated in \autoref{chapter:sota} and \autoref{section:cpmodel}, LNS is used to expand the exploration
of the search tree. We now compare our solver with the use of LNS and without. We also compare multiple 
relaxations method.


\autoref{experiments:lns} shows the performance profile for the final objective after 30s of search 
between a standard search and LNS. As expected, we can see that LNS outperforms the standard search 
in every instances. \autoref{experiments:lns-oot} shows that the standard search quickly get stuck
while the LNS manages to find better solutions quickly after relaxing the initial solution.


\begin{figure}
  \centering
  \includegraphics[scale=0.55]{experiments/lns-profile.png}
  \caption{CP with and without Large Neighborhood Search [216 instances/30s].}
  \label{experiments:lns}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[scale=0.55]{experiments/lns-oot.png}
  \caption{CP with and without Large Neighborhood Search [216 instances/3s].}
  \label{experiments:lns-oot}
\end{figure}



We also experimented with multiple relaxations in the LNS framework. We compared:

\begin{itemize}
  \item Random relaxation with 40\% relaxation (CP-Random-40)
  \item Random relaxation with 50\% relaxation (CP-Random-50)
  \item Random relaxation with 60\% relaxation (CP-Random-60)
  \item Random relaxation with 70\% relaxation (CP-Random-70)
  \item Random relaxation with 80\% relaxation (CP-Random-80)
  \item Propagation based relaxation (CP-Prop)
\end{itemize}


\autoref{experiments:relax-profile} shows the performance profile for the objective after 30s with 
CP-Prop as baseline.
We can observe that overall, CP-Random-40 finds a better objective in 80\% of instances. However,
no relaxation finds a better solution in every instances. 


\autoref{experiments:relax-profile} shows that in average CP-Random-40 is the fastest to find a solution close 
to the best one with CP-Random-50 close behind.

\begin{figure}
  \centering
  \includegraphics[scale=0.55]{experiments/relax-profile.png}
  \caption{Comparison between multiple relaxations [216 instances/30s].}
  \label{experiments:relax-profile}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[scale=0.55]{experiments/relax-oot.png}
  \caption{Comparison between multiple relaxations [216 instances/30s].}
  \label{experiments:relax-oot}
\end{figure}




\section{Comparison between solvers}

We now start by comparing different solvers together. We experiment with three solvers: 

\begin{itemize}
  \item The Constraint Programming (CP) solver.
  \item The Mixed Integer Programming (MIP) solver.
  \item A combination of CP and MIP (CP+MIP) solvers. We take an initial solution from the CP solver 
  and give it to the MIP solver as start solution. 
\end{itemize}



\autoref{experiments:solvers:profile} 
shows a performance profile with the CP solver as baseline. We can clearly see that 
the MIP solver underperforms for most instance. However, it outperforms the CP solver for approximately
20\% of instances. [TODO insert figure] shows that MIP has trouble finding good objectives when the 
problem grows. Too many variables (\char`\~ 1,000,000 binary variables for the biggest instances) make it hard for a MIP model to perform well.

We can also observe from that performance profile, and in \autoref{experiments:solvers:profile2}, that 
the CP+MIP solver gives the best objective in 60\% of instances. For the remaining instances, the objective is only worse by maximum 10\%. 
The MIP solver performs a lot better when given an initial solution to work with.

\begin{figure}
  \centering
  \includegraphics[scale=0.55]{experiments/solvers-profile.png}
  \caption{CP, MIP and CP+MIP solvers [216 instances/30s].}
  \label{experiments:solvers:profile}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[scale=0.55]{experiments/cp,cp+mip-profile.png}
  \caption{CP, MIP and CP+MIP solvers [216 instances/30s].}
  \label{experiments:solvers:profile2}
\end{figure}



\autoref{experiments:solvers:oot} gives the objective per time for the three solvers. As expected from our previous results, the MIP solver does not 
give a good solution after the elapsed time. However, while the CP+MIP solver is slightly better in terms of final objective, it takes in average 10 seconds 
to reach such objective while the CP solver only takes 1 second to reach an objective really close to the final one.

% Those 20\% match with the small instances proportions in \autoref{instances:proportions}


\begin{figure}
  \centering
  \includegraphics[scale=0.55]{experiments/solvers-oot.png}
  \caption{CP, MIP and CP+MIP solvers [216 instances/30s].}
  \label{experiments:solvers:oot}
\end{figure}


\begin{figure}
  \centering
  \includegraphics[scale=0.55]{experiments/time-first-sol.png}
  \caption{Time on first solution [216 instances/First solution].}
  \label{experiments:first-sol-time}
\end{figure}


\begin{figure}
  \centering
  \includegraphics[scale=0.55]{experiments/obj-first-sol.png}
  \caption{Objective on first solution [216 instances/First solution].}
  \label{experiments:first-sol-obj}
\end{figure}




\end{document}

